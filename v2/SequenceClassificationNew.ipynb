{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from v2.data_transformers.SequenceAttritionTrsTransformerNew import SequenceAttritionTrsTransformerNew\n",
    "\n",
    "current_month = '05'\n",
    "\n",
    "input_paths = {\n",
    "    'data': f'<path_to_file>full_till_{current_month}.2023.xlsx',\n",
    "    'training': f'<path_to_file>.xlsx'\n",
    "}\n",
    "\n",
    "outputh_paths = {\n",
    "    'general':'<output_path>'\n",
    "}\n",
    "\n",
    "cleanup = False\n",
    "transformer = SequenceAttritionTrsTransformerNew(path_to_data=input_paths['data'],\n",
    "                                                 training_data_path=input_paths['training'],\n",
    "                                                 predict=False,\n",
    "                                                 cleanup=False)\n",
    "\n",
    "data = transformer.prepare_data_for_attrition_prediction(min_date='01.01.2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import unidecode\n",
    "finance['Employee'] = finance['Employee'].apply(lambda x: unidecode.unidecode(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Setting date to last day of month due to different days of nTRS extract and align with FINANCE data.\n",
    "data['report_date_dt'] = pd.to_datetime(data['report_date_dt'], format='%Y-%m-%d') + MonthEnd(0)\n",
    "finance['change_date'] = pd.to_datetime(finance['change_date'], format='%Y-%m-%d') + MonthEnd(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "data = data.merge(finance, how='left', left_on=['employee', 'report_date_dt'], right_on=['Employee', 'change_date'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "#data = data.drop(['SAP ID', 'change_date', 'Employee'], axis=1)\n",
    "data = data.rename(columns={'change_value': 'salary_raise'})\n",
    "data['salary_raise'] = data['salary_raise'].fillna(0.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "data = data.drop(['Employee', 'change_date', 'SAP ID'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# Split the dataset for training, backtest and prediction\n",
    "#backtest_date = '2023-04-28'\n",
    "prediction_start_date = '2023-01-31'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "data = data[data['report_date_dt'] < prediction_start_date]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [name, surname, grade, technology, office location, client, current project, sap id, line manager, employee, unique_id, report_date_dt, start_date_dt, max_date, mapped_grade, left, office_location_prob_ratio, technology_prob_ratio, mapped_client_prob_ratio, has_training, salary_raise]\nIndex: []\n\n[0 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>surname</th>\n      <th>grade</th>\n      <th>technology</th>\n      <th>office location</th>\n      <th>client</th>\n      <th>current project</th>\n      <th>sap id</th>\n      <th>line manager</th>\n      <th>employee</th>\n      <th>...</th>\n      <th>report_date_dt</th>\n      <th>start_date_dt</th>\n      <th>max_date</th>\n      <th>mapped_grade</th>\n      <th>left</th>\n      <th>office_location_prob_ratio</th>\n      <th>technology_prob_ratio</th>\n      <th>mapped_client_prob_ratio</th>\n      <th>has_training</th>\n      <th>salary_raise</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n<p>0 rows × 21 columns</p>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['report_date_dt'] > prediction_start_date]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "2022-10-31    1234\n2022-11-30    1229\n2022-09-30    1219\n2022-12-31    1210\n2022-07-31    1207\n2022-08-31    1206\n2022-06-30    1185\n2022-05-31    1161\n2022-04-30    1152\n2022-02-28    1131\n2022-01-31    1126\n2022-03-31    1124\n2021-12-31    1107\n2021-11-30    1097\n2021-10-31    1061\n2021-09-30    1034\n2021-08-31     987\n2021-07-31     963\n2021-06-30     929\n2021-05-31     910\n2021-04-30     864\n2021-03-31     816\n2021-02-28     803\n2021-01-31     791\n2020-11-30     778\n2020-12-31     777\n2020-10-31     767\n2020-09-30     729\n2020-07-31     709\n2020-06-30     708\n2020-05-31     707\n2020-08-31     704\n2020-04-30     684\n2020-03-31     662\n2020-02-29     652\n2020-01-31     643\n2018-09-30     632\n2018-08-31     626\n2018-07-31     621\n2019-12-31     617\n2019-11-30     614\n2018-06-30     611\n2018-10-31     599\n2019-10-31     599\n2018-05-31     597\n2018-11-30     580\n2019-09-30     572\n2018-04-30     571\n2018-12-31     569\n2019-01-31     563\n2019-02-28     545\n2019-08-31     540\n2018-03-31     538\n2019-03-31     524\n2018-02-28     506\n2019-07-31     503\n2019-04-30     500\n2019-06-30     491\n2019-05-31     488\n2018-01-31     476\nName: report_date_dt, dtype: int64"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.report_date_dt.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "(18476, 21)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terminated = data[data['left'] == 1]\n",
    "terminated.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['name', 'surname', 'grade', 'technology', 'office location', 'client',\n       'current project', 'sap id', 'line manager', 'employee', 'unique_id',\n       'report_date_dt', 'start_date_dt', 'max_date', 'mapped_grade', 'left',\n       'office_location_prob_ratio', 'technology_prob_ratio',\n       'mapped_client_prob_ratio', 'has_training', 'salary_raise'],\n      dtype='object')"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terminated.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dateutil import relativedelta\n",
    "\n",
    "\n",
    "def calc_2(x, y):\n",
    "    delta = relativedelta.relativedelta(x, y)\n",
    "    return delta.months\n",
    "\n",
    "\n",
    "def calc_month_before_termination(data):\n",
    "    data['months_before_termination'] = ((data['max_date'] - data['report_date_dt']) / np.timedelta64(1, 'M'))\n",
    "    data['months_before_termination'] = data['months_before_termination'].apply(lambda x: round(x))\n",
    "    return data\n",
    "\n",
    "\n",
    "def calc_month_of_work(data):\n",
    "    # todo fix this calculation\n",
    "    data['months_of_work'] = ((data['report_date_dt'] - data['start_date_dt']) / np.timedelta64(1, 'M'))\n",
    "    data['months_of_work'] = data['months_of_work'].apply(lambda x: round(x))\n",
    "    return data\n",
    "\n",
    "\n",
    "def calculate_risk(x):\n",
    "    if x <= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def set_sample(x, y, intervals: int = 2):\n",
    "    if x == 1:  # always take high risk periods\n",
    "        return True\n",
    "\n",
    "    if x == np.nan: # those are mid-periods we shouldn't consider\n",
    "        return False\n",
    "\n",
    "    if x == 0:  # low risk periods\n",
    "        if y % intervals == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "def is_on_bench(x, y):\n",
    "    # x = project, y = client\n",
    "    x = x.lower()\n",
    "    y = y.lower()\n",
    "    if 'bench' in x or 'bench' in y:\n",
    "        return 1\n",
    "    elif 'internal' in y:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def avg_time_per_project(tenure, nth_project):\n",
    "    return tenure / nth_project\n",
    "\n",
    "\n",
    "def set_prob_period(x):\n",
    "    if x < 4:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_tenure(x, y):\n",
    "    months_of_empl = (y - x) / np.timedelta64(1, 'M')\n",
    "    return months_of_empl / 12\n",
    "\n",
    "def is_covid_employment(start_date):\n",
    "    import datetime\n",
    "    covid_start = datetime.datetime.strptime('20-03-2020', '%d-%m-%Y')\n",
    "    covid_end = datetime.datetime.strptime('13-05-2022', '%d-%m-%Y')\n",
    "    if covid_start < start_date < covid_end:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_time_to_event(date_column, event_column):\n",
    "    return (risky[date_column] - risky.groupby(['unique_id', risky[event_column].eq(1).cumsum()])[date_column].transform(\"min\"))/np.timedelta64(1, 'M')\n",
    "\n",
    "def get_time_to_event_on_float(date_column, event_column):\n",
    "    return (risky[date_column] - risky.groupby(['unique_id', risky[event_column].gt(0).cumsum()])[date_column].transform(\"min\"))/np.timedelta64(1, 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18476, 25)\n",
      "(15737, 25)\n"
     ]
    }
   ],
   "source": [
    "#terminated['months_before_termination'] = terminated.apply(lambda x: calc_2(x['max_date'], x['report_date_dt']), axis=1)\n",
    "terminated['tenure'] = terminated.apply(lambda x: get_tenure(x['start_date_dt'], x['report_date_dt']), axis=1)\n",
    "terminated = calc_month_before_termination(terminated)\n",
    "terminated['high_risk'] = terminated['months_before_termination'].apply(calculate_risk)\n",
    "terminated = calc_month_of_work(terminated)\n",
    "print(terminated.shape)\n",
    "terminated['is_prob'] = terminated['months_of_work'].apply(set_prob_period)\n",
    "terminated = terminated[terminated['is_prob'] == False]\n",
    "terminated.drop(['is_prob'], axis=1, inplace=True)\n",
    "print(terminated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "risky = terminated.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0    13783\n1     1954\nName: high_risk, dtype: int64"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risky['high_risk'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "risky.drop(['left', 'diff'], inplace=True, axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "risky['current project'] = risky['current project'].astype(str)\n",
    "risky['client'] = risky['client'].astype(str)\n",
    "risky['current project'] = risky['current project'].str.lower()\n",
    "risky['client'] = risky['client'].str.lower()\n",
    "risky['is_on_bench'] = risky.apply(lambda x: is_on_bench(x['current project'], x['client']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%% todo one function with dynamic column names\n"
    }
   },
   "outputs": [],
   "source": [
    "risky.sort_values(['unique_id', 'report_date_dt'], inplace=True)\n",
    "risky['was_promoted'] = risky.groupby('unique_id').apply(\n",
    "    lambda group: group['grade'] != group['grade'].shift(1)).tolist()\n",
    "risky['was_promoted'] = risky['was_promoted'].map({True: 1, False: 0})\n",
    "risky.loc[risky.groupby('unique_id').head(1).index, 'was_promoted'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "risky['project_changed'] = risky.groupby('unique_id').apply(\n",
    "    lambda group: group['current project'] != group['current project'].shift(1)).tolist()\n",
    "risky['project_changed'] = risky['project_changed'].map({True: 1, False: 0})\n",
    "risky.loc[risky.groupby('unique_id').head(1).index, 'project_changed'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "risky['account_changed'] = risky.groupby('unique_id').apply(\n",
    "    lambda group: group['client'] != group['client'].shift(1)).tolist()\n",
    "risky['account_changed'] = risky['account_changed'].map({True: 1, False: 0})\n",
    "risky.loc[risky.groupby('unique_id').head(1).index, 'account_changed'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0    13783\n1     1954\nName: high_risk, dtype: int64"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risky['high_risk'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "risky[\"time_since_promo_in_months\"] = get_time_to_event(date_column='report_date_dt', event_column='was_promoted')\n",
    "risky[\"time_since_project_change_in_months\"] = get_time_to_event(date_column='report_date_dt', event_column='project_changed')\n",
    "risky[\"time_since_account_change_in_months\"] = get_time_to_event(date_column='report_date_dt', event_column='account_changed')\n",
    "risky[\"time_since_bench_in_months\"] = get_time_to_event(date_column='report_date_dt', event_column='is_on_bench')\n",
    "risky[\"time_since_training_in_months\"] = get_time_to_event(date_column='report_date_dt', event_column='has_training')\n",
    "risky['time_since_last_salary_raise'] = get_time_to_event_on_float(date_column='report_date_dt', event_column='salary_raise')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# populate last salary raise\n",
    "risky['salary_raise_test'] = risky['salary_raise'].replace(0.00, np.nan)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "risky.loc[risky.groupby('unique_id').head(1).index, 'salary_raise_test'] = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "risky['last_salary_raise'] = risky['salary_raise_test'].ffill()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "risky = risky.drop(['salary_raise_test'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risky['last_salary_raise'].isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "risky.sort_values(['unique_id', 'report_date_dt'], inplace=True)\n",
    "risky['bench_cumsum'] = risky.groupby(['unique_id'])['is_on_bench'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "risky['bench_to_tenure'] = risky.apply(lambda x: (x['bench_cumsum'] / 12) / x['tenure'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "risky['nth_project'] = risky.groupby(['unique_id'])[\n",
    "                           'project_changed'].cumsum() + 1  # +1 beacuse it is reflecting the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "risky['avg_time_per_project'] = risky.apply(lambda x : avg_time_per_project(x['tenure'], x['nth_project']), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "risky = risky.drop(['is_on_bench', 'has_training'], axis=1)\n",
    "risky['sampling'] = risky.apply(lambda x: set_sample(x['high_risk'], x['months_of_work']), axis=1)\n",
    "risky = risky[risky['sampling'] == True]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "risky.drop(['months_before_termination', 'sampling','months_of_work', 'grade', 'mapped_source', 'report_date', 'bench_cumsum',\n",
    "            'left', 'diff', 'initial_grade', 'max_report_date_all', 'max_date', 'technology', 'start_date_dt',\n",
    "            'report_date_dt', 'employee', 'Jjob family', 'source', 'status', 'division', 'technology', 'start date',\n",
    "            'office location', 'contract type', 'client', 'current project', 'new_job_family', 'name', 'surname', 'grade', 'last_project', 'last_client', 'mapped_project', 'mapped_client', 'has_training', 'salary_raise',\n",
    "            'is_on_bench', 'was_promoted', 'project_changed', 'account_changed', 'line manager'], axis=1, inplace=True,\n",
    "           errors='ignore')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8969, 18)\n",
      "Index(['sap id', 'unique_id', 'mapped_grade', 'office_location_prob_ratio',\n",
      "       'technology_prob_ratio', 'mapped_client_prob_ratio', 'tenure',\n",
      "       'high_risk', 'time_since_promo_in_months',\n",
      "       'time_since_project_change_in_months',\n",
      "       'time_since_account_change_in_months', 'time_since_bench_in_months',\n",
      "       'time_since_training_in_months', 'time_since_last_salary_raise',\n",
      "       'last_salary_raise', 'bench_to_tenure', 'nth_project',\n",
      "       'avg_time_per_project'],\n",
      "      dtype='object')\n",
      "0    7015\n",
      "1    1954\n",
      "Name: high_risk, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(risky.shape)\n",
    "print(risky.columns)\n",
    "print(risky['high_risk'].value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "time_lagged_columns = [\n",
    "    'time_since_promo_in_months',\n",
    "    'time_since_project_change_in_months',\n",
    "    'time_since_account_change_in_months',\n",
    "    'time_since_bench_in_months',\n",
    "    'time_since_training_in_months',\n",
    "    'time_since_last_salary_raise'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 34.99it/s]\n"
     ]
    }
   ],
   "source": [
    "columns_based_on_lagged_features = []\n",
    "periods = [(0,3), (3,6), (6, 12)]\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for column in tqdm(time_lagged_columns):\n",
    "    for period in periods:\n",
    "        base_period = period[0]\n",
    "        desired_period = period[1]\n",
    "        new_col = column.replace('time_since_', '')\n",
    "        new_col = new_col.replace('_in_months', '')\n",
    "        columns_based_on_lagged_features.append(new_col)\n",
    "        risky[f'{new_col}_{desired_period}_m'] = risky[column].apply(lambda x: 1 if base_period <= x < desired_period else 0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "changed_risky = risky.copy(deep=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "# CORR\n",
    "risky = risky.drop(time_lagged_columns, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "# VIF to verify m-coli\n",
    "verify_coli = False\n",
    "if verify_coli:\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "    # VIF dataframe\n",
    "    vif_data = pd.DataFrame()\n",
    "    target = risky['high_risk']\n",
    "    X = risky.drop(['high_risk', 'unique_id'], axis=1) # remove unique_id ONLY for VIF\n",
    "    assert 'high_risk' not in X.columns\n",
    "    vif_data[\"feature\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n",
    "                              for i in range(len(X.columns))]\n",
    "\n",
    "    print(vif_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "risky = risky[risky['mapped_grade'] != '7'] # remove L7 grade from classification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "risky = risky.drop(['office_location_prob_ratio'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8969 entries, 32896 to 26218\n",
      "Data columns (total 30 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   sap id                      1237 non-null   object \n",
      " 1   unique_id                   8969 non-null   object \n",
      " 2   mapped_grade                8969 non-null   int64  \n",
      " 3   office_location_prob_ratio  8969 non-null   float64\n",
      " 4   technology_prob_ratio       8969 non-null   float64\n",
      " 5   mapped_client_prob_ratio    8969 non-null   float64\n",
      " 6   tenure                      8969 non-null   float64\n",
      " 7   high_risk                   8969 non-null   int64  \n",
      " 8   last_salary_raise           8969 non-null   float64\n",
      " 9   bench_to_tenure             8969 non-null   float64\n",
      " 10  nth_project                 8969 non-null   int64  \n",
      " 11  avg_time_per_project        8969 non-null   float64\n",
      " 12  promo_3_m                   8969 non-null   int64  \n",
      " 13  promo_6_m                   8969 non-null   int64  \n",
      " 14  promo_12_m                  8969 non-null   int64  \n",
      " 15  project_change_3_m          8969 non-null   int64  \n",
      " 16  project_change_6_m          8969 non-null   int64  \n",
      " 17  project_change_12_m         8969 non-null   int64  \n",
      " 18  account_change_3_m          8969 non-null   int64  \n",
      " 19  account_change_6_m          8969 non-null   int64  \n",
      " 20  account_change_12_m         8969 non-null   int64  \n",
      " 21  bench_3_m                   8969 non-null   int64  \n",
      " 22  bench_6_m                   8969 non-null   int64  \n",
      " 23  bench_12_m                  8969 non-null   int64  \n",
      " 24  training_3_m                8969 non-null   int64  \n",
      " 25  training_6_m                8969 non-null   int64  \n",
      " 26  training_12_m               8969 non-null   int64  \n",
      " 27  last_salary_raise_3_m       8969 non-null   int64  \n",
      " 28  last_salary_raise_6_m       8969 non-null   int64  \n",
      " 29  last_salary_raise_12_m      8969 non-null   int64  \n",
      "dtypes: float64(7), int64(21), object(2)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "risky.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "risky = risky.drop(['line manager', 'sap id', 'tenure'], axis=1, errors='ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "col_to_pairplot = [\n",
    "    'time_since_last_salary_raise',\n",
    "    'time_since_promo_in_months',\n",
    "    # 'time_since_account_change_in_months',\n",
    "    # 'time_since_bench_in_months',\n",
    "    # 'time_since_training_in_months',\n",
    "    # 'bench_to_tenure',\n",
    "    # 'avg_time_per_project',\n",
    "    # 'nth_project',\n",
    "    'high_risk'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#sns.pairplot(risky[col_to_pairplot], hue='high_risk')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "#sns.boxplot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "splitter = GroupShuffleSplit(test_size=.20, n_splits=5, random_state=7)\n",
    "split = splitter.split(risky, groups=risky['unique_id'])\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "X_train = risky.iloc[train_inds]\n",
    "X_test = risky.iloc[test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['unique_id', 'mapped_grade', 'office_location_prob_ratio',\n       'technology_prob_ratio', 'mapped_client_prob_ratio', 'tenure',\n       'high_risk', 'last_salary_raise', 'bench_to_tenure', 'nth_project',\n       'avg_time_per_project', 'promo_3_m', 'promo_6_m', 'promo_12_m',\n       'project_change_3_m', 'project_change_6_m', 'project_change_12_m',\n       'account_change_3_m', 'account_change_6_m', 'account_change_12_m',\n       'bench_3_m', 'bench_6_m', 'bench_12_m', 'training_3_m', 'training_6_m',\n       'training_12_m', 'last_salary_raise_3_m', 'last_salary_raise_6_m',\n       'last_salary_raise_12_m'],\n      dtype='object')"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# X_train = risky.groupby('unique_id').sample(frac=.8)\n",
    "# print(risky.shape)\n",
    "# x_train_indexes = X_train.index\n",
    "# print(x_train_indexes.shape)\n",
    "# X_test = risky.loc[~risky.index.isin(x_train_indexes)]\n",
    "# print(X_test.shape)\n",
    "\n",
    "# print(unique_id.ngroups)\n",
    "# print(X_test.groupby('unique_id').ngroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train.drop(['unique_id'], axis=1, inplace=True, errors='ignore')\n",
    "X_test.drop(['unique_id'], axis=1, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# corr = X_train.corr()\n",
    "# sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_train = X_train['high_risk']\n",
    "X_train.drop(['high_risk'], axis=1, inplace=True)\n",
    "\n",
    "y_test = X_test['high_risk']\n",
    "X_test.drop(['high_risk'], axis=1, inplace=True)\n",
    "\n",
    "assert 'high_risk' not in X_train.columns\n",
    "assert 'high_risk' not in X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7171, 25)\n",
      "(7171,)\n",
      "--test--\n",
      "(1798,)\n",
      "(1798, 25)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('--test--')\n",
    "print(y_test.shape)\n",
    "print(X_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "A, a = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
    " n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 5613, 1: 1558})\n",
      "Counter({0: 1402, 1: 396})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter_train = Counter(y_train)\n",
    "counter_test = Counter(y_test)\n",
    "print(counter_train)\n",
    "print(counter_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "# from numpy import where\n",
    "# from matplotlib import pyplot\n",
    "#\n",
    "# for label, _ in counter_train.items():\n",
    "#     row_ix = where(y_train == label)[0]\n",
    "#     pyplot.scatter(X_train[row_ix, 0], X_train[row_ix, 1], label=str(label))\n",
    "# pyplot.legend()\n",
    "# pyplot.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "outlier_detection = False\n",
    "\n",
    "if outlier_detection:\n",
    "    from pyod.models.knn import KNN\n",
    "    from pyod.models.lof import LOF\n",
    "    from pyod.models.copod import COPOD\n",
    "    from pyod.models.iforest import IForest\n",
    "\n",
    "    from pyod.utils.example import visualize\n",
    "    from pyod.utils.data import evaluate_print\n",
    "\n",
    "    clf_name = 'KNN'\n",
    "\n",
    "    from pyod.models.suod import SUOD\n",
    "\n",
    "# initialized a group of outlier detectors for acceleration\n",
    "    detector_list = [LOF(n_neighbors=15), LOF(n_neighbors=20),\n",
    "                    LOF(n_neighbors=25), LOF(n_neighbors=35),\n",
    "                    COPOD(), IForest(n_estimators=100),\n",
    "                    IForest(n_estimators=200)]\n",
    "\n",
    "# decide the number of parallel process, and the combination method\n",
    "# then clf can be used as any outlier detection model\n",
    "    clf = SUOD(base_estimators=detector_list, n_jobs=2, combination='average',\n",
    "           verbose=False)\n",
    "\n",
    "    clf.fit(X_train)\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "# get the prediction on the test data\n",
    "    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "    y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "# it is possible to get the prediction confidence as well\n",
    "    #y_test_pred, y_test_pred_confidence = clf.predict(X_test, return_confidence=True)  # outlier labels (0 or 1) and confidence in the range of [0,1]\n",
    "\n",
    "\n",
    "# evaluate and print the results\n",
    "    print(\"\\nOn Training Data:\")\n",
    "    evaluate_print(clf_name, y_train, y_train_scores)\n",
    "    print(\"\\nOn Test Data:\")\n",
    "    evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "    #visualize(clf_name, X_train, y_train, X_test, y_test, y_train_pred,\n",
    "    #      y_test_pred, show_figure=True, save_figure=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Specs      Score\n",
      "6     avg_time_per_project  64.303836\n",
      "19            training_3_m  34.306661\n",
      "9               promo_12_m  20.594446\n",
      "21           training_12_m  19.403919\n",
      "7                promo_3_m  17.749316\n",
      "22   last_salary_raise_3_m  17.230134\n",
      "16               bench_3_m  16.233155\n",
      "15     account_change_12_m  11.889392\n",
      "24  last_salary_raise_12_m  11.856418\n",
      "13      account_change_3_m  11.045296\n",
      "12     project_change_12_m   9.415622\n",
      "18              bench_12_m   8.375247\n",
      "10      project_change_3_m   7.400209\n",
      "3        last_salary_raise   6.871663\n",
      "23   last_salary_raise_6_m   4.163264\n",
      "0             mapped_grade   3.945869\n",
      "20            training_6_m   3.415049\n",
      "5              nth_project   3.166826\n",
      "8                promo_6_m   1.048696\n",
      "11      project_change_6_m   0.650967\n"
     ]
    }
   ],
   "source": [
    "# FEATURE SELECTION\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "#X_train_fs = SelectKBest(score_func=chi2, k='all').fit_transform(X_train,y_train)\n",
    "\n",
    "bestfeatures = SelectKBest(score_func=f_classif, k=20)\n",
    "fit = bestfeatures.fit(X_train,y_train)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X_train.columns)\n",
    "#concat two dataframes for better visualization\n",
    "featureScores = pd.concat([dfcolumns, dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(20,'Score'))  #print 10 best features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "['avg_time_per_project',\n 'training_3_m',\n 'promo_12_m',\n 'training_12_m',\n 'promo_3_m']"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_rated_features = featureScores.sort_values(['Score'], ascending=False)['Specs'].head(5).tolist()\n",
    "top_rated_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "X_train = X_train[top_rated_features]\n",
    "X_test = X_test[top_rated_features]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:07<00:00,  3.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from lazypredict.Supervised import LazyClassifier\n",
    "clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = clf.fit(X_train, X_test, y_train, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "data": {
      "text/plain": "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\nModel                                                                           \nXGBClassifier                      0.76               0.67     0.67      0.77   \nLGBMClassifier                     0.75               0.65     0.65      0.75   \nRandomForestClassifier             0.72               0.64     0.64      0.73   \nExtraTreeClassifier                0.73               0.64     0.64      0.74   \nKNeighborsClassifier               0.74               0.64     0.64      0.74   \nExtraTreesClassifier               0.73               0.64     0.64      0.74   \nBaggingClassifier                  0.72               0.63     0.63      0.73   \nDecisionTreeClassifier             0.72               0.63     0.63      0.73   \nQuadraticDiscriminantAnalysis      0.54               0.55     0.55      0.58   \nNearestCentroid                    0.51               0.55     0.55      0.55   \nGaussianNB                         0.50               0.54     0.54      0.54   \nAdaBoostClassifier                 0.72               0.54     0.54      0.70   \nSVC                                0.70               0.53     0.53      0.69   \nPassiveAggressiveClassifier        0.67               0.53     0.53      0.67   \nLabelSpreading                     0.68               0.53     0.53      0.68   \nSGDClassifier                      0.67               0.53     0.53      0.67   \nBernoulliNB                        0.67               0.53     0.53      0.67   \nCategoricalNB                      0.66               0.52     0.52      0.67   \nLabelPropagation                   0.68               0.52     0.52      0.68   \nRidgeClassifier                    0.74               0.52     0.52      0.70   \nRidgeClassifierCV                  0.74               0.52     0.52      0.70   \nLinearSVC                          0.74               0.52     0.52      0.70   \nLinearDiscriminantAnalysis         0.74               0.52     0.52      0.69   \nLogisticRegression                 0.74               0.51     0.51      0.69   \nCalibratedClassifierCV             0.74               0.51     0.51      0.69   \nPerceptron                         0.76               0.50     0.50      0.69   \nDummyClassifier                    0.78               0.50     0.50      0.68   \nNuSVC                              0.38               0.49     0.49      0.41   \n\n                               Time Taken  \nModel                                      \nXGBClassifier                        0.18  \nLGBMClassifier                       0.11  \nRandomForestClassifier               0.52  \nExtraTreeClassifier                  0.01  \nKNeighborsClassifier                 0.07  \nExtraTreesClassifier                 0.42  \nBaggingClassifier                    0.06  \nDecisionTreeClassifier               0.02  \nQuadraticDiscriminantAnalysis        0.01  \nNearestCentroid                      0.01  \nGaussianNB                           0.01  \nAdaBoostClassifier                   0.12  \nSVC                                  1.67  \nPassiveAggressiveClassifier          0.01  \nLabelSpreading                       1.07  \nSGDClassifier                        0.02  \nBernoulliNB                          0.01  \nCategoricalNB                        0.01  \nLabelPropagation                     0.77  \nRidgeClassifier                      0.01  \nRidgeClassifierCV                    0.02  \nLinearSVC                            0.21  \nLinearDiscriminantAnalysis           0.02  \nLogisticRegression                   0.02  \nCalibratedClassifierCV               0.95  \nPerceptron                           0.01  \nDummyClassifier                      0.01  \nNuSVC                                1.23  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accuracy</th>\n      <th>Balanced Accuracy</th>\n      <th>ROC AUC</th>\n      <th>F1 Score</th>\n      <th>Time Taken</th>\n    </tr>\n    <tr>\n      <th>Model</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>XGBClassifier</th>\n      <td>0.76</td>\n      <td>0.67</td>\n      <td>0.67</td>\n      <td>0.77</td>\n      <td>0.18</td>\n    </tr>\n    <tr>\n      <th>LGBMClassifier</th>\n      <td>0.75</td>\n      <td>0.65</td>\n      <td>0.65</td>\n      <td>0.75</td>\n      <td>0.11</td>\n    </tr>\n    <tr>\n      <th>RandomForestClassifier</th>\n      <td>0.72</td>\n      <td>0.64</td>\n      <td>0.64</td>\n      <td>0.73</td>\n      <td>0.52</td>\n    </tr>\n    <tr>\n      <th>ExtraTreeClassifier</th>\n      <td>0.73</td>\n      <td>0.64</td>\n      <td>0.64</td>\n      <td>0.74</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>KNeighborsClassifier</th>\n      <td>0.74</td>\n      <td>0.64</td>\n      <td>0.64</td>\n      <td>0.74</td>\n      <td>0.07</td>\n    </tr>\n    <tr>\n      <th>ExtraTreesClassifier</th>\n      <td>0.73</td>\n      <td>0.64</td>\n      <td>0.64</td>\n      <td>0.74</td>\n      <td>0.42</td>\n    </tr>\n    <tr>\n      <th>BaggingClassifier</th>\n      <td>0.72</td>\n      <td>0.63</td>\n      <td>0.63</td>\n      <td>0.73</td>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>DecisionTreeClassifier</th>\n      <td>0.72</td>\n      <td>0.63</td>\n      <td>0.63</td>\n      <td>0.73</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>QuadraticDiscriminantAnalysis</th>\n      <td>0.54</td>\n      <td>0.55</td>\n      <td>0.55</td>\n      <td>0.58</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>NearestCentroid</th>\n      <td>0.51</td>\n      <td>0.55</td>\n      <td>0.55</td>\n      <td>0.55</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>GaussianNB</th>\n      <td>0.50</td>\n      <td>0.54</td>\n      <td>0.54</td>\n      <td>0.54</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>AdaBoostClassifier</th>\n      <td>0.72</td>\n      <td>0.54</td>\n      <td>0.54</td>\n      <td>0.70</td>\n      <td>0.12</td>\n    </tr>\n    <tr>\n      <th>SVC</th>\n      <td>0.70</td>\n      <td>0.53</td>\n      <td>0.53</td>\n      <td>0.69</td>\n      <td>1.67</td>\n    </tr>\n    <tr>\n      <th>PassiveAggressiveClassifier</th>\n      <td>0.67</td>\n      <td>0.53</td>\n      <td>0.53</td>\n      <td>0.67</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>LabelSpreading</th>\n      <td>0.68</td>\n      <td>0.53</td>\n      <td>0.53</td>\n      <td>0.68</td>\n      <td>1.07</td>\n    </tr>\n    <tr>\n      <th>SGDClassifier</th>\n      <td>0.67</td>\n      <td>0.53</td>\n      <td>0.53</td>\n      <td>0.67</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>BernoulliNB</th>\n      <td>0.67</td>\n      <td>0.53</td>\n      <td>0.53</td>\n      <td>0.67</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>CategoricalNB</th>\n      <td>0.66</td>\n      <td>0.52</td>\n      <td>0.52</td>\n      <td>0.67</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>LabelPropagation</th>\n      <td>0.68</td>\n      <td>0.52</td>\n      <td>0.52</td>\n      <td>0.68</td>\n      <td>0.77</td>\n    </tr>\n    <tr>\n      <th>RidgeClassifier</th>\n      <td>0.74</td>\n      <td>0.52</td>\n      <td>0.52</td>\n      <td>0.70</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>RidgeClassifierCV</th>\n      <td>0.74</td>\n      <td>0.52</td>\n      <td>0.52</td>\n      <td>0.70</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>LinearSVC</th>\n      <td>0.74</td>\n      <td>0.52</td>\n      <td>0.52</td>\n      <td>0.70</td>\n      <td>0.21</td>\n    </tr>\n    <tr>\n      <th>LinearDiscriminantAnalysis</th>\n      <td>0.74</td>\n      <td>0.52</td>\n      <td>0.52</td>\n      <td>0.69</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>LogisticRegression</th>\n      <td>0.74</td>\n      <td>0.51</td>\n      <td>0.51</td>\n      <td>0.69</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>CalibratedClassifierCV</th>\n      <td>0.74</td>\n      <td>0.51</td>\n      <td>0.51</td>\n      <td>0.69</td>\n      <td>0.95</td>\n    </tr>\n    <tr>\n      <th>Perceptron</th>\n      <td>0.76</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.69</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>DummyClassifier</th>\n      <td>0.78</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.68</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>NuSVC</th>\n      <td>0.38</td>\n      <td>0.49</td>\n      <td>0.49</td>\n      <td>0.41</td>\n      <td>1.23</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n",
      "Best params: {'gamma': 0, 'learning_rate': 0.03, 'max_depth': 8, 'n_estimators': 800, 'reg_alpha': 0.1, 'reg_lambda': 0}\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from numpy import mean, std\n",
    "\n",
    "\n",
    "xgb_params = {\n",
    "        'gamma': [0, 0.1, 0.2, 0.4],\n",
    "        'learning_rate': [0.01, 0.03],\n",
    "        'max_depth': [5, 6, 7, 8],\n",
    "        'n_estimators': [800],\n",
    "        'reg_alpha': [0, 0.1, 0.2],\n",
    "        'reg_lambda': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "lgbm_params = {\n",
    "    'boosting_type': ['gbdt', 'goss', 'rf', 'dart'],\n",
    "    'learning_rate':[0.001, 0.01, 0.1],\n",
    "    'n_estimators': [50],\n",
    "    'max_depth':[10, 15, 20, 25],\n",
    "    'num_leaves': [30, 35, 40, 50]\n",
    "}\n",
    "\n",
    "lgbm = LGBMClassifier(random_state=42)\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator=xgb, param_grid=xgb_params, scoring='recall', cv=3,\n",
    "                        n_jobs=-1, verbose=2)\n",
    "\n",
    "result = grid.fit(X_train, y_train)\n",
    "best_classifier = result.best_estimator_\n",
    "best_params = result.best_params_\n",
    "print(f'Best params: {best_params}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "# OVER SAMPLING ON TRAINING DATASET\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "use_smote = False\n",
    "\n",
    "if use_smote:\n",
    "\n",
    "    counter_train = Counter(y_train)\n",
    "    print(counter_train)\n",
    "    print('-' * 20)\n",
    "\n",
    "    over = SMOTE(sampling_strategy=0.4)\n",
    "    under = RandomUnderSampler(sampling_strategy=0.7)\n",
    "\n",
    "    steps = [('o', over), ('u', under)]\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    X_train_smote, y_train_smote = pipeline.fit_resample(X_train, y_train)\n",
    "    counter_train = Counter(y_train_smote)\n",
    "    print(counter_train)\n",
    "\n",
    "    X_train = X_train_smote\n",
    "    y_train = y_train_smote"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
      "              grow_policy='depthwise', importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.03, max_bin=256,\n",
      "              max_cat_threshold=64, max_cat_to_onehot=4, max_delta_step=0,\n",
      "              max_depth=8, max_leaves=0, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=800, n_jobs=0,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=42, ...)\n"
     ]
    }
   ],
   "source": [
    "print(best_classifier)\n",
    "model = best_classifier\n",
    "# model_with_best_params = LGBMClassifier(learning_rate=0.1, boosting_type='gbdt', max_depth=20, n_estimators=1000, num_leaves=35,random_state=42)\n",
    "\n",
    "#steps = [('model', model_with_best_params)]\n",
    "#pipeline = Pipeline(steps=steps)\n",
    "cv = RepeatedStratifiedKFold(n_splits=7, n_repeats=3, random_state=42)\n",
    "recalls = cross_val_score(model, X_train, y_train, scoring='recall', cv=cv, n_jobs=-1)\n",
    "roc_curve = cross_val_score(model, X_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "precisions = cross_val_score(model, X_train, y_train, scoring='precision', cv=cv, n_jobs=-1)\n",
    "accuracy = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print(f'Accuracy: {mean(accuracy)} with std: {std(accuracy)}')\n",
    "print(f'Precision: {mean(precisions)} with std: {std(precisions)}')\n",
    "print(f'Recalls: {mean(recalls)} with std: {std(recalls)}')\n",
    "print(f'ROC: {mean(roc_curve)} with std: {std(roc_curve)}')\n",
    "# model_with_best_params_2 = LGBMClassifier(learning_rate=0.1, boosting_type='gbdt', max_depth=20, n_estimators=1400, num_leaves=35,random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_prob = model.predict_proba(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21456454200113928\n"
     ]
    }
   ],
   "source": [
    "# KAPPA\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "print(cohen_kappa_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# BRIER SCORE\n",
    "from sklearn.metrics import brier_score_loss\n",
    "brier_score_loss(y_test, y_pred_prob[:, 1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aaa"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_artifacts = True\n",
    "base_output_path = 'artifacts'\n",
    "data_output = 'data'\n",
    "model_output = 'models'\n",
    "\n",
    "import uuid\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "artifacts_id = str(uuid.uuid4())\n",
    "print(f'{artifacts_id}')\n",
    "data_path = f'{base_output_path}/{artifacts_id}/{data_output}'\n",
    "model_path = f'{base_output_path}/{artifacts_id}/{model_output}'\n",
    "if save_artifacts:\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "        os.makedirs(model_path)\n",
    "    X_train.to_csv(f'{data_path}/training_data.csv', sep=';', index=0)\n",
    "    with open(f'{model_path}/model.joblib', 'wb') as file:\n",
    "        joblib.dump(model, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_df = X_test.reset_index()\n",
    "base_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from explainerdashboard import ClassifierExplainer, ExplainerDashboard\n",
    "\n",
    "\n",
    "explainer = ClassifierExplainer(model, X_test, y_test,\n",
    "                                labels=['low risk', 'high risk'], # defaults to ['0', '1', etc]\n",
    "                                )\n",
    "\n",
    "db = ExplainerDashboard(explainer,\n",
    "                        title=\"KIT Explainer\", # defaults to \"Model Explainer\"\n",
    "                        shap_interaction=False, # you can switch off tabs with bools\n",
    "                        whatif=True,\n",
    "                        decision_trees=False\n",
    "                        )\n",
    "db.run(port=8050)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer(X_test)\n",
    "shap.plots.beeswarm(shap_values) # wez ten wykres, jest super"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# shap.force_plot(explainer.expected_value, shap_values[0, :], X_test.iloc[0, :])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "feat_importances.sort_values(ascending=False).head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Explainer - LIME"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime import lime_tabular\n",
    "\n",
    "explainer = lime_tabular.LimeTabularExplainer(\n",
    "    training_data=np.array(X_train),\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=['low risk', 'high risk'],\n",
    "    mode='classification'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "exp = explainer.explain_instance(\n",
    "        data_row=X_test.iloc[126],\n",
    "        predict_fn=model.predict_proba\n",
    "    )\n",
    "individual_contributors = exp.as_list()\n",
    "print(individual_contributors)\n",
    "# individual_contributors = dict((re.sub('[^a-zA-Z_]', '', key), val) for (key, val) in individual_contributors)\n",
    "# temp_dict = {50: individual_contributors}\n",
    "# contrib_single_df = pd.DataFrame(temp_dict).transpose().reset_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.predict_proba(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp.show_in_notebook(show_table=True, show_all=False, show_predicted_value=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_test.iloc[126]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def prepare_contributors_for_prediction(pred_df: pd.DataFrame, classifier):\n",
    "    all_contributors = []\n",
    "    n_predictions = pred_df.shape[0]\n",
    "    print(f'Processing {n_predictions} predictions')\n",
    "    for i in range(n_predictions):\n",
    "        exp = explainer.explain_instance(\n",
    "            data_row=pred_df.iloc[i],\n",
    "            predict_fn=classifier.predict_proba\n",
    "        )\n",
    "        individual_contributors = exp.as_list()\n",
    "        print(individual_contributors)\n",
    "        individual_contributors = dict((re.sub('[^a-zA-Z_]', '', key), val) for (key, val) in individual_contributors)\n",
    "        temp_dict = {i: individual_contributors}\n",
    "        contrib_single_df = pd.DataFrame(temp_dict).transpose().reset_index()\n",
    "        all_contributors.append(contrib_single_df)\n",
    "    print(f'Processed successfully {len(all_contributors)} predictions')\n",
    "    return all_contributors\n",
    "\n",
    "contributors = pd.concat(prepare_contributors_for_prediction(X_test, model), axis=0, ignore_index=True)\n",
    "contributors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "contributors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_train = pipeline.predict(X_train)\n",
    "print(classification_report(y_train, y_pred_train))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ggg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "recalls = cross_val_score(model, X_test, y_test, scoring='recall', cv=cv, n_jobs=-1)\n",
    "roc_curve = cross_val_score(model, X_test, y_test, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "precisions = cross_val_score(model, X_test, y_test, scoring='precision', cv=cv, n_jobs=-1)\n",
    "accuracy = cross_val_score(model, X_test, y_test, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print(f'Accuracy: {mean(accuracy)} with std: {std(accuracy)}')\n",
    "print(f'Precision: {mean(precisions)} with std: {std(precisions)}')\n",
    "print(f'Recalls: {mean(recalls)} with std: {std(recalls)}')\n",
    "print(f'ROC: {mean(roc_curve)} with std: {std(roc_curve)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "RocCurveDisplay.from_estimator(model, X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dt_new = DecisionTreeClassifier(class_weight='balanced')\n",
    "rm_new = RandomForestClassifier(class_weight='balanced')\n",
    "knn = KNeighborsClassifier()\n",
    "xgb = XGBClassifier()#scale_pos_weight=scale_pos_weight)\n",
    "\n",
    "\n",
    "models_params_grid = {\n",
    "    'KNN':\n",
    "        {'n_neighbors': [3, 7, 9, 11, 13, 15, 17, 19],\n",
    "         'weights': ['uniform', 'distance'],\n",
    "         'p': [1, 2]\n",
    "         },\n",
    "    'DecisionTree': {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [2, 4, 6, 8, 10, 12],\n",
    "        'max_leaf_nodes': [2, 4, 6, 8, 10, 12]\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'bootstrap': [True, False],\n",
    "        'max_depth': [10, 20, 30, 40],\n",
    "        'max_features': ['auto', 'sqrt'],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'n_estimators': [200, 400, 600]\n",
    "    },\n",
    "    'Random Survival Forest' :{\n",
    "        'n_estimators': [200, 400, 600, 800, 1000],\n",
    "        'min_samples_split': [10,20,30, 40],\n",
    "        'min_samples_leaf': [5,10,15,20]\n",
    "    },\n",
    "    'Xgboost': {\n",
    "        'gamma': [0, 0.1, 0.2, 0.4],\n",
    "        'learning_rate': [0.01, 0.03],\n",
    "        'max_depth': [5, 6, 7, 8],\n",
    "        'n_estimators': [150, 300],\n",
    "        'reg_alpha': [0, 0.1, 0.2],\n",
    "        'reg_lambda': [0, 0.1, 0.2]\n",
    "    }\n",
    "}\n",
    "\n",
    "for model, model_name in zip([knn, dt_new, rm_new],\n",
    "                             ['KNN', 'DecisionTree', 'Random Forest']):\n",
    "    print(f'****')\n",
    "    print(f'{model_name} started...')\n",
    "\n",
    "    grid = GridSearchCV(estimator=model, param_grid=models_params_grid.get(model_name), scoring='recall', cv=3,\n",
    "                        n_jobs=-1, verbose=1)\n",
    "    result = grid.fit(X_train, y_train)\n",
    "    best_classifier = result.best_estimator_\n",
    "    print(best_classifier)\n",
    "    steps = [('model', best_classifier)]\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=7, n_repeats=3, random_state=42)\n",
    "    recalls = cross_val_score(pipeline, X_train, y_train, scoring='recall', cv=cv, n_jobs=-1)\n",
    "    roc_curve = cross_val_score(pipeline, X_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "    precisions = cross_val_score(pipeline, X_train, y_train, scoring='precision', cv=cv, n_jobs=-1)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    #print(roc_curve)\n",
    "    probablities = pipeline.predict_proba(X_test)[:, 1]\n",
    "    df = pd.DataFrame({'y_pred_class': y_pred,\n",
    "                       'y_test': y_test,\n",
    "                       'proba': probablities})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay, RocCurveDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions = best_classifier.predict(X_test)\n",
    "\n",
    "\n",
    "PrecisionRecallDisplay.from_predictions(y_test, predictions)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, predictions)\n",
    "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
    "roc = RocCurveDisplay.from_predictions(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "persist = False\n",
    "\n",
    "if persist:\n",
    "    import joblib, uuid\n",
    "    model_id = str(uuid.uuid4())\n",
    "    model_dir = 'models'\n",
    "    file_name = f'{model_dir}/model_{model_id}.joblib'\n",
    "    with open(file_name, 'wb') as file:\n",
    "        joblib.dump(best_classifier, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "risky.drop(['unique_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#sns.pairplot(risky, hue='high_risk')\n",
    "risky.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "risky['tenure'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = risky.columns\n",
    "\n",
    "# calculating VIF for each feature\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(risky.values, i)\n",
    "                   for i in range(len(risky.columns))]\n",
    "\n",
    "print(vif_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
